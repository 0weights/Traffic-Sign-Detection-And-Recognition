{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classifier","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fCyPC-tcl8So","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Download data set 'DONE'{ form-width: \"10%\" }\n","!pip install -q kaggle\n","from google.colab import files\n","# files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n","!unzip /content/gtsrb-german-traffic-sign.zip -d '/content/Model 1 resourses'\n","# !cp -r '/content/Model 1 resourses' '/content/drive/My Drive'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYXkBlYfvBMt","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Mount Drive{ form-width: \"10%\" }\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlUR_lAKv9RP","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Get Data From Drive{ form-width: \"10%\" }\n","!cp -r \"/content/drive/My Drive/GTSRB_new\" /content\n","!ln -s /content/drive/My\\ Drive/ /mydrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxOEGSVAqSar","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Libraries{ form-width: \"10%\" }\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import skimage as sk\n","import pandas as pd\n","import numpy as np\n","import subprocess\n","import pickle\n","import cv2\n","import os\n","\n","from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, concatenate, Concatenate, Input, merge, Activation\n","from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n","from keras.models import Model, Sequential, load_model\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import SGD, Adam, RMSprop\n","from keras.initializers import glorot_uniform\n","from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n","from keras.utils import CustomObjectScope\n","from keras.utils import plot_model\n","from keras import backend as k\n","\n","from sklearn.model_selection import train_test_split\n","from skimage import morphology, filters, io, transform, color, exposure\n","from random import shuffle\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAHgfIChwUnz","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Hyperparametars{ form-width: \"10%\" }\n","\n","# Paths\n","TRAIN_DIR = \"/content/Model 1 resourses/Train\"\n","TEST_DIR = \"/content/Model 1 resourses/Test\"\n","\n","train_data_npy = '/content/drive/My Drive/GTSRB_new/train_data_hybird_Inception.npy'\n","train_data_label_npy = '/content/drive/My Drive/GTSRB_new/train_data_label_hybird_Inception.npy'\n","\n","test_data_npy = '/content/drive/My Drive/GTSRB_new/test_data_hybird_Inception.npy'\n","test_data_label_npy = '/content/drive/My Drive/GTSRB_new/test_data_label_hybird_Inception.npy'\n","\n","validate_data_npy = '/content/drive/My Drive/GTSRB_new/validate_data_hybird_Inception.npy'\n","validate_data_label_npy = '/content/drive/My Drive/GTSRB_new/validate_data_label_hybird_Inception.npy'\n","\n","date = '6.8.' # 6.8 means 6/8/2020\n","A = 'A.' # we changed data augmentatino variables\n","B = 'B.' # Hybird Inception\n","O = 'O.' # original Inception\n","C = 'C.' # using normalization in pre-processing\n","M = 'M.' # functional model\n","\n","model_name = date + M + 'InceptionWithDataAaugmentationEpoch200'\n","model_summary = ''\n","gtsrb_path =  '/content/drive/My Drive/GTSRB_new'\n","model_path = '/content/GTSRB_new/97.8939P1/6.8.M.InceptionWithoutDataAaugmentationEpoch30.h5'\n","\n","csv_path = \"/content/drive/My Drive/GTSRB_new/Test.csv\"\n","\n","classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\n","           \"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\n","           \"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\n","           \"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\n","           \"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\n","           \"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\n","           \"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\"\n","           ]\n","\n","IMG_SIZE = 48\n","# IMG_SIZE = 32\n","# how many samples to calculte the error\n","BATCH_SIZE = 16\n","\n","## Flags\n","# 0 for vgg, 1 for inception, 2 for hybird inception\n","Classifire_flag = 2\n","Test_flag = 1\n","# 1 for force \n","Force_train_flag = 1\n","Normalize_Flag = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MueOpSmmwqLG","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Image Preprocessing Old{ form-width: \"10%\" }\n","def histogram_equalize(image):\n","    kernel =  sk.morphology.disk(30)\n","    img_local = sk.filters.rank.equalize(image, selem=kernel)\n","    return img_local\n","\n","def create_label(folder_name):\n","    \"\"\" Create an one-hot encoded vector from folder name \"\"\"\n","    index = classes.index(folder_name)\n","    return np.array([1 if i == index else 0 for i in range(43)])\n","\n","def create_train_data():\n","    training_data = []\n","    for folder in tqdm(classes):\n","        folder_path = TRAIN_DIR + \"/\" + folder\n","        for train_img in os.listdir(folder_path):\n","            img_path = os.path.join(folder_path, train_img)\n","            train_img_data = cv2.imread(img_path, 0)\n","            train_img_data = histogram_equalize(train_img_data)\n","            train_img_data = cv2.resize(train_img_data, (IMG_SIZE, IMG_SIZE)) / 255.0\n","            training_data.append([np.array(train_img_data), create_label(folder)])\n","    shuffle(training_data)\n","    np.save('train_data_vgg.npy', training_data)\n","    return training_data\n","\n","def create_test_data():\n","    testing_data = []\n","    print('pre-processing test data')\n","    for test_img in (os.listdir(TEST_DIR)):\n","        img_path = os.path.join(TEST_DIR, test_img)\n","        if test_img == \"GT-final_test.csv\":\n","          continue\n","        test_img_data = cv2.imread(img_path, 0)\n","        test_img_data = histogram_equalize(test_img_data)\n","        test_img_data = cv2.resize(test_img_data, (IMG_SIZE, IMG_SIZE)) / 255.0\n","        testing_data.append([np.array(test_img_data), test_img])\n","    np.save('test_data_vgg.npy', testing_data)\n","    return testing_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yT9VyZXwV0G1","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Image Preprocessing New{ form-width: \"10%\" }\n","def process(img):\n","  if Normalize_Flag == 1:\n","    # Histogram normalization in y\n","    hsv = color.rgb2hsv(img)\n","    hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n","    img = color.hsv2rgb(hsv)\n","\n","  min_side = min(img.shape[:-1])\n","  centre = img.shape[0]//2, img.shape[1]//2\n","  img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n","            centre[1]-min_side//2:centre[1]+min_side//2,:]\n","  img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n","  return img\n","\n","def create_label(folder_name):\n","  \"\"\" Create an one-hot encoded vector from folder name \"\"\"\n","  index = classes.index(folder_name)\n","  return np.array([1 if i == index else 0 for i in range(43)])\n","\n","def create_data():\n","  training_data = []\n","  training_data_label = []\n","  print('pre-processing train data')\n","\n","  training_images_path = []\n","  for folder in tqdm(classes):\n","    folder_path = TRAIN_DIR + \"/\" + folder\n","    for train_img in os.listdir(folder_path):\n","      training_images_path.append([os.path.join(folder_path, train_img), folder])\n","  shuffle(training_images_path)\n","\n","  for img_path, folder in tqdm(training_images_path):\n","    train_img_data = process(io.imread(img_path))\n","    training_data.append(train_img_data)\n","    training_data_label.append(create_label(folder))\n","\n","  testing_data = []\n","  testing_data_label = []\n","  print('pre-processing test data')\n","  for test_img in (os.listdir(TEST_DIR)):\n","    img_path = os.path.join(TEST_DIR, test_img)\n","    if test_img == \"GT-final_test.csv\":\n","      continue\n","    test_img_data = process(io.imread(img_path))\n","    testing_data.append(test_img_data)\n","    testing_data_label.append(test_img)\n","\n","  training_data = np.array(training_data, dtype='float32')\n","  training_data_label = np.array(training_data_label)\n","\n","  testing_data = np.array(testing_data, dtype='float32')\n","  testing_data_label = np.array(testing_data_label)\n","\n","  index=np.zeros(1307,  dtype='int')\n","  for i in range(1307):\n","    index[i]=i*30+np.random.randint(0,30) \n","  \n","  validation_data = training_data[index]\n","  validation_data_label = training_data_label[index]\n","  \n","  # creat the training index1\n","  new_index=np.setdiff1d(np.array(range(39209)), index, assume_unique=True)\n","  training_data=training_data[new_index]\n","  training_data_label=training_data_label[new_index]\n","\n","  mean_image = np.mean(training_data, axis=0)\n","  training_data -= mean_image\n","  testing_data -= mean_image\n","  validation_data -= mean_image\n","\n","  \n","  np.save('normalized_train_data_hybird_Inception.npy', training_data)\n","  np.save('normalized_train_data_label_hybird_Inception.npy', training_data_label)\n","  np.save('normalized_test_data_hybird_Inception.npy', testing_data)\n","  np.save('normalized_test_data_label_hybird_Inception.npy', testing_data_label)\n","  np.save('normalized_validate_data_hybird_Inception.npy', validation_data)\n","  np.save('normalized_validate_data_label_hybird_Inception.npy', validation_data_label)\n","  return training_data,training_data_label,testing_data,testing_data_label,validation_data,validation_data_label\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VBjKCx6zYbf","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Training Plots{ form-width: \"10%\" }\n","\n","def plots(history):\n","    # Plot history\n","    # Plot training & validation accuracy values\n","    print(history.history.keys())\n","    plt.plot(history.history['accuracy'] , '-o')\n","    plt.plot(history.history['val_accuracy'], '-x')\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validate'], loc='lower right')\n","    fig = plt.gcf()\n","    fig.set_size_inches(25,8)\n","    plt.savefig(model_name + '.acc.png')\n","    plt.show()\n","\n","    plt.plot(history.history['loss'], '-o')\n","    plt.plot(history.history['val_loss'], '-x')\n","    plt.title('Loss for Chennai Reservoir Levels')\n","    plt.ylabel('Loss value')\n","    plt.xlabel('No. epoch')\n","    plt.legend(['Loss (training data)', 'Loss (validation data)'], loc=\"upper right\")\n","    fig = plt.gcf()\n","    fig.set_size_inches(25,8)\n","    plt.savefig(model_name + '.loss.png')\n","    plt.show()\n","    print(history.history.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiDCM3SfxNGy","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Hybrid VGG{ form-width: \"10%\" }\n","def hybrid_vgg():\n","    model = Sequential()\n","    # model.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n","    model.add(Conv2D(input_shape=(IMG_SIZE,IMG_SIZE,1),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n","    model.add(BatchNormalization())\n","\n","    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(rate=0.5))\n","\n","    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(rate=0.5))\n","\n","    model.add(Flatten())\n","    model.add(Dense(units=256,activation=\"relu\"))\n","    model.add(Dropout(rate=0.5))\n","    model.add(Dense(units=43, activation=\"softmax\"))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mE3nWuNGysst","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Inception { form-width: \"5%\" }\n","\n","def new_classifire():\n","    input_img = Input(shape=(48, 48, 1))\n","\n","    layer_1 = Conv2D(32, (3,3), padding='same', activation='relu')(input_img)\n","    layer_2 = Conv2D(48, (7,1), padding='same', activation='relu')(layer_1)\n","    layer_3 = Conv2D(48, (1,7), padding='same', activation='relu')(layer_2)\n","    max_1 = MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\")(layer_3)\n","    drop_1 = Dropout(rate=0.2)(max_1)\n","\n","    inc_1 = Conv2D(64, (3,1), padding='same', activation='relu')(drop_1)\n","    inc_1 = Conv2D(64, (1,3), padding='same', activation='relu')(inc_1)\n","    inc_2 = Conv2D(64, (1,7), padding='same', activation='relu')(drop_1)\n","    inc_2 = Conv2D(64, (7,1), padding='same', activation='relu')(inc_2)\n","    concatinate = concatenate([inc_1, inc_2], axis = -1)\n","    max_2 = MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\")(concatinate)\n","    drop_2 = Dropout(rate=0.2)(max_2)\n","\n","    layer_4 = Conv2D(128, (3,3), padding='same', activation='relu')(drop_2)\n","    layer_5 = Conv2D(256, (3,3), padding='same', activation='relu')(layer_4)\n","    max_3 = MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\")(layer_5)\n","    drop_3 = Dropout(rate=0.3)(max_3)\n","\n","    flat_1 = Flatten()(drop_3)\n","    dense_1 = Dense(256, activation='relu')(flat_1)\n","    drop_4 = Dropout(rate=0.4)(dense_1)\n","    output = Dense(43, activation='softmax')(drop_4)\n","\n","    model = Model([input_img], output)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUo9bc7KUB6L","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Hybird Inception (functional API ) { form-width: \"5%\" }\n","\n","def functional_hybird_inception():\n","  input_img = Input(shape=(48, 48, 3))\n","\n","  layer_1 = Conv2D(32, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(input_img)\n","  layer_1_b = BatchNormalization(epsilon=1e-06, axis=3)(layer_1)\n","  layer_2 = Conv2D(48, (7,1), padding='same', activation='relu' ,kernel_initializer='he_normal')(layer_1_b)\n","  layer_2_b = BatchNormalization(epsilon=1e-06, axis=3)(layer_2)\n","  layer_3 = Conv2D(48, (1,7), padding='same', activation='relu' ,kernel_initializer='he_normal')(layer_2_b)\n","  layer_3_b = BatchNormalization(epsilon=1e-06, axis=3)(layer_3)\n","  max_1 = MaxPool2D(pool_size=(2,2))(layer_3_b)\n","  drop_1 = Dropout(rate=0.2)(max_1)\n","\n","  inc_1 = Conv2D(64, (3,1), padding='same', activation='relu' ,kernel_initializer='he_normal')(drop_1)\n","  inc_1 = BatchNormalization(epsilon=1e-06, axis=3)(inc_1)\n","  inc_1 = Conv2D(64, (1,3), padding='same', activation='relu' ,kernel_initializer='he_normal')(inc_1)\n","  inc_1 = BatchNormalization(epsilon=1e-06, axis=3)(inc_1)\n","\n","  inc_2 = Conv2D(64, (1,7), padding='same', activation='relu' ,kernel_initializer='he_normal')(drop_1)\n","  inc_2 = BatchNormalization(epsilon=1e-06, axis=3)(inc_2)\n","  inc_2 = Conv2D(64, (7,1), padding='same', activation='relu' ,kernel_initializer='he_normal')(inc_2)\n","  inc_2 = BatchNormalization(epsilon=1e-06, axis=3)(inc_2)\n","  ######\n","  merge_layer = concatenate([inc_1, inc_2])\n","  ######\n","  max_2 = MaxPool2D(pool_size=(2,2))(merge_layer)\n","  drop_2 = Dropout(rate=0.2)(max_2)\n","\n","  layer_4 = Conv2D(128, (3,3), padding='same', activation='relu' ,kernel_initializer='he_normal')(drop_2)\n","  layer_4_b = BatchNormalization(epsilon=1e-06, axis=3)(layer_4)\n","  layer_5 = Conv2D(256, (3,3), padding='same', activation='relu' ,kernel_initializer='he_normal')(layer_4_b)\n","  layer_5_b = BatchNormalization(epsilon=1e-06, axis=3)(layer_5)\n","\n","  max_3 = MaxPool2D(pool_size=(2,2))(layer_5_b)\n","  drop_3 = Dropout(rate=0.3)(max_3)\n","\n","  flat_1 = Flatten()(drop_3)\n","  dense_1 = Dense(256, activation='relu', kernel_initializer='he_normal')(flat_1)\n","  ldense_1_b = BatchNormalization()(dense_1)\n","  drop_4 = Dropout(rate=0.4)(ldense_1_b)\n","  output = Dense(43, activation='softmax', kernel_initializer='he_normal')(drop_4)\n","\n","  model = Model([input_img], output)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qw0A03yBLKs","colab_type":"code","cellView":"form","colab":{}},"source":["#@title original (sequential API){ form-width: \"10%\" }\n","def original():\n","  branch_0= Sequential()\n","  branch_1 = Sequential()\n","  model0 = Sequential()\n","  model = Sequential()\n","  # ********************************************** 48*48\n","  model0.add(Convolution2D(32, 3, 3, border_mode='same', init='he_normal' , input_shape=(48, 48, 3)))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  model0.add(Activation('relu'))\n","\n","  model0.add(Convolution2D(48, 7, 1, border_mode='same', init='he_normal'))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  model0.add(Activation('relu'))\n","  model0.add(Convolution2D(48, 1, 7, border_mode='same', init='he_normal'))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  model0.add(Activation('relu'))   \n","  model0.add(MaxPooling2D(pool_size=(2, 2)))\n","  model0.add(Dropout(0.2))\n","  # ****************************************** 24*24\n","  branch_0.add(model0)\n","  branch_1.add(model0)\n","  \n","  branch_0.add(Convolution2D(64, 3, 1, border_mode='same', init='he_normal'))\n","  branch_0.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  branch_0.add(Activation('relu'))\n","  branch_0.add(Convolution2D(64, 1, 3, border_mode='same', init='he_normal'))\n","  branch_0.add(BatchNormalization(epsilon=1e-06,  axis=3))\n","  branch_0.add(Activation('relu'))\n","  \n","  branch_1.add(Convolution2D(64, 1, 7, border_mode='same', init='he_normal'))\n","  branch_1.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  branch_1.add(Activation('relu'))\n","  branch_1.add(Convolution2D(64, 7, 1, border_mode='same', init='he_normal'))\n","  branch_1.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  branch_1.add(Activation('relu'))    \n","  \n","  model.add(Merge([branch_0, branch_1], mode='concat', concat_axis=-1))\n","  \n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.2))\n","  \n","  # ******************************************* 12*12\n","  model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal'))\n","  model.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  \n","  model.add(Activation('relu'))\n","  model.add(Convolution2D(256, 3, 3, border_mode='same', init='he_normal'))   # 之前是256个滤波器\n","  model.add(BatchNormalization(epsilon=1e-06, axis=3))\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.3))\n","  \n","  # *************************************** 6*6\n","  model.add(Flatten())\n","  model.add(Dense(256, init='he_normal'))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.4))\n","  model.add(Dense(43, activation='softmax', init='he_normal'))\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4UCkzvjiVqr","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Hyberd Inception (sequential & functional API){ form-width: \"2%\" }\n","def sequential_hyperd_inception():\n","  model0 = Sequential()\n","  model0._name = 'tree_base'\n","  branch_0 = Sequential()\n","  branch_0._name = 'branch0'\n","  branch_1 = Sequential()\n","  branch_1._name = 'branch1'\n","\n","  \n","  # 48*48\n","  # IMG_SIZE = 48\n","  model0.add(Conv2D(input_shape=(IMG_SIZE, IMG_SIZE, 3), filters=32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal', name='conv_1'))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_1'))\n","  model0.add(Activation('relu', name='active_1'))\n","  model0.add(Conv2D(filters=48, kernel_size=(7, 1), padding='same', kernel_initializer='he_normal', name='conv_2'))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_2'))\n","  model0.add(Activation('relu',name='active_2'))\n","  model0.add(Conv2D(filters=48, kernel_size=(1, 7), padding='same', kernel_initializer='he_normal', name='conv_3'))\n","  model0.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_3'))\n","  model0.add(Activation('relu', name='active_3'))\n","  model0.add(MaxPooling2D(pool_size=(2, 2), name='max_1'))\n","  model0.add(Dropout(0.2, name='droup_1'))\n","\n","  # 24*24\n","  branch_0.add(model0)\n","  branch_1.add(model0)\n"," \n","\n","  branch_0.add(Conv2D(filters=64, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal', name='conv_3'))\n","  branch_0.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_4'))\n","  branch_0.add(Activation('relu', name='active_4'))\n","  branch_0.add(Conv2D(filters=64, kernel_size=(1, 3), padding='same', kernel_initializer='he_normal' , name='conv_4'))\n","  branch_0.add(BatchNormalization(epsilon=1e-06,  axis=3, name='batch_5'))\n","  branch_0.add(Activation('relu', name='active_5'))\n","\n","  branch_1.add(Conv2D(filters=64, kernel_size=(1, 7), padding='same', kernel_initializer='he_normal' , name='conv_5'))\n","  branch_1.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_6'))\n","  branch_1.add(Activation('relu', name='active_6'))\n","  branch_1.add(Conv2D(filters=64, kernel_size=(7, 1), padding='same', kernel_initializer='he_normal', name='conv_6' ))\n","  branch_1.add(BatchNormalization(epsilon=1e-06, axis=3, name='batch_7'))\n","  branch_1.add(Activation('relu', name='active_7'))\n","\n","  merged = concatenate([branch_0.output, branch_1.output])\n","  merged = MaxPooling2D(pool_size=(2, 2))(merged)\n","  merged = Dropout(0.2)(merged)\n","  \n","  # 12*12\n","  merged = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal' )(merged)\n","  merged = BatchNormalization(epsilon=1e-06, axis=3)(merged)\n","  merged = Activation('relu')(merged)\n","  \n","  merged = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal' )(merged)\n","  merged = BatchNormalization(epsilon=1e-06, axis=3)(merged)\n","  merged = Activation('relu')(merged)\n","  merged = MaxPooling2D(pool_size=(2, 2))(merged)\n","  merged = Dropout(0.3)(merged)\n","  \n","  #  6*6\n","  merged = Flatten()(merged)\n","  merged = Dense(256, kernel_initializer='he_normal')(merged)\n","  merged = BatchNormalization()(merged)\n","  merged = Dropout(0.4)(merged)\n","  merged = Dense(43, activation='softmax', kernel_initializer='he_normal')(merged)\n","  modelMerged = Model(outputs=merged)\n","  return modelMerged"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwEonTsJDXuB","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Main{ form-width: \"2%\" }\n","\n","if os.path.exists(train_data_npy):\n","  X_train = np.load(train_data_npy, allow_pickle=True)\n","  y_train = np.load(train_data_label_npy, allow_pickle=True)\n","  X_test = np.load(test_data_npy, allow_pickle=True)\n","  y_test = np.load(test_data_label_npy, allow_pickle=True)\n","  X_validate = np.load(validate_data_npy, allow_pickle=True)\n","  y_validate = np.load(validate_data_label_npy, allow_pickle=True)\n","  print(\"All Data Loaded Successfully\")\n","  print('--------------------------------')\n","else:\n","    X_train,y_train,X_test,y_test,X_validate,y_validate = create_data()\n","\n","print ('Train data shape: ', X_train.shape)\n","print ('Train labels shape: ', y_train.shape)\n","print ('Validation data shape: ', X_validate.shape)\n","print ('Validation labels shape: ', y_validate.shape)\n","print ('Test data shape: ', X_test.shape)\n","print ('Test labels shape: ', y_test.shape)\n","\n","\n","# test the classifire\n","# before training test history\n","\n","if Classifire_flag == 0:\n","  model = hybrid_vgg()\n","elif Classifire_flag == 1:\n","  model = new_classifire()\n","else:\n","  model = functional_hybird_inception()\n","\n","if os.path.exists(model_path) and not Force_train_flag:\n","  with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n","    model = load_model(model_path)\n","  print(\"Loaded model from disk\")\n","  print('--------------------------------')\n","else:\n","  if os.path.exists(model_path):\n","    model = load_model(model_path)\n","    print(\"Loaded model from disk\")\n","    print('--------------------------------')\n","  print(\"Start Training\")\n","  print('--------------------------------')\n","  model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=Adam(lr=0.001, decay=1e-6))\n","  # save the model\n","\n","  dataGen= ImageDataGenerator(featurewise_center=False, \n","                              featurewise_std_normalization=False, \n","                              width_shift_range=0.1,\n","                              height_shift_range=0.1,\n","                              zoom_range=0.2,\n","                              shear_range=0.1,\n","                              rotation_range=10.,)\n","  \n","  dataGen.fit(X_train) \n","  history=model.fit_generator(dataGen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n","                            steps_per_epoch=len(X_train) // BATCH_SIZE,\n","                            epochs=200,\n","                            validation_data=(X_validate,y_validate),\n","                            callbacks=[ReduceLROnPlateau('val_loss', factor=0.2, patience=20, verbose=1, mode='auto'), \n","                                       ModelCheckpoint(model_name + '.h5',save_best_only=True)]\n","                           )\n","  # history=model.fit_generator(dataGen.flow(X_train, y_train,batch_size=BATCH_SIZE),steps_per_epoch= len(xtrain) // BATCH_SIZE,epochs=200,validation_data=(xvalidate,yvalidate))\n","  # history = model.fit(x=X_train, y=y_train, epochs=30, batch_size=BATCH_SIZE,validation_data=(X_validate, y_validate),shuffle = True)\n","\n","  # model.save(model_name + '.h5')\n","  plots(history)\n","  with open(model_name + '.txt', 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)\n","  print(\"Saved model to disk\")\n","  print('--------------------------------')\n","\n","# testing\n","if Test_flag:\n","  print(\"Start Testing\")\n","  print('--------------------------------')  \n","  fields = [\"Path\", \"ClassId\"]\n","  data = pd.read_csv(csv_path,  usecols=fields)\n","  test_original = data.to_numpy()\n","  row , col = test_original.shape\n","  accepted_prediction = 0\n","  for img_counter in range(len(X_test)):\n","    test_img_name = 'Test/' + y_test[img_counter]\n","    x_test_4d= np.expand_dims(X_test[img_counter], axis=0)\n","    prediction = model.predict(x_test_4d)\n","    for i in range(row):\n","      if test_original[i][1] == test_img_name:\n","        class_value = test_original[i][0]\n","        break\n","    if(np.argmax(prediction) == class_value):\n","      accepted_prediction += 1\n","  acc = (accepted_prediction/row) * 100\n","  print(\"final acc is {}\".format(acc))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bN6Nb5y89Sxa","colab_type":"code","cellView":"form","colab":{}},"source":["# @title Move Model Resources{ form-width: \"10%\" }\n","# path_resources = '/mydrive/GTSRB_new' + '/' + str(round(history.history['val_accuracy'][0], 4))\n","path_resources = '/mydrive/GTSRB_new' + '/' + str(round(acc, 4))\n","\n","path_matiral_1 = '/content/' + model_name + '.h5'\n","path_matiral_2 = '/content/' + model_name + '.txt'\n","path_matiral_3 = '/content/' + model_name + '.acc.png'\n","path_matiral_4 = '/content/' + model_name + '.loss.png'\n","\n","subprocess.call(['bash','./script.sh',path_resources,\n","                 path_matiral_1,\n","                 path_matiral_2,\n","                 path_matiral_3,\n","                 path_matiral_4])\n","\n","!mkdir $path_resources\n","!cp  $path_matiral_1 $path_resources\n","!cp  $path_matiral_2 $path_resources\n","!cp  $path_matiral_3 $path_resources\n","!cp  $path_matiral_4 $path_resources"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zdbgl_2D3mJp","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Show Model Summary{ form-width: \"10%\" }\n","history = pickle.load(open(model_summary, \"rb\"))\n","for variavle in history:\n","  print(variavle + ': ' , history[variavle])\n","len(model.layers)\n","model.summary()"],"execution_count":null,"outputs":[]}]}